"""
@date: 2025
@version: 0.3.1
@description: ç”¨äºæ‰«æAPIç«¯ç‚¹
"""

import random
import requests, re
from urllib.parse import urlparse
from bs4 import BeautifulSoup
import argparse
import time
import sys
import json
import os
from datetime import datetime
from ua_manager import UaManager
from utils import URLProcessor, URLExtractor
from i18n import i18n
import threading
import pyfiglet
from rich.console import Console
from rich.text import Text
from rich.panel import Panel
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn
from rich.align import Align

parser = argparse.ArgumentParser(description="Api-Finder v0.3")
parser.add_argument("-u", "--url", help=i18n.get('arg_url_help'), required=True)
parser.add_argument("-c", "--cookie", help=i18n.get('arg_cookie_help'))
parser.add_argument("-p", "--proxy", help=i18n.get('arg_proxy_help'))
parser.add_argument("-s", "--silent", action="store_true", help=i18n.get('arg_silent_help'))
parser.add_argument("-o", "--output", help=i18n.get('arg_output_help'))
parser.add_argument("-t", "--timeout", type=int, default=10, help=i18n.get('arg_timeout_help'))
parser.add_argument("-d", "--delay", type=float, default=0.5, help=i18n.get('arg_delay_help'))
parser.add_argument("-v", "--verbose", action="store_true", help=i18n.get('arg_verbose_help'))
parser.add_argument("-r", "--random", action="store_true", help=i18n.get('arg_random_help'))
parser.add_argument("-a", "--app", help=i18n.get('arg_app_help'), default='common')
arg = parser.parse_args()

# åˆå§‹åŒ–Rich Console (Initialize Rich Console)
console = Console()

# åˆå§‹åŒ–UAç®¡ç†å™¨ (Initialize UA Manager)
Uam = UaManager(arg.app, arg.random)

# ä½¿ç”¨Riché‡æ„çš„Logoæ˜¾ç¤º
def show_logo():
	"""ä½¿ç”¨Richå’Œpyfigletæ˜¾ç¤ºç²¾ç¾logo"""
	try:
		# ç”ŸæˆASCII art
		logo_text = pyfiglet.figlet_format("Api-Finder", font="slant")
		
		# åˆ›å»ºå¸¦é¢œè‰²çš„logoæ–‡æœ¬
		logo = Text(logo_text, style="cyan bold")
		
		# åˆ›å»ºé¡¹ç›®ä¿¡æ¯æ–‡æœ¬
		info_text = Text()
		info_text.append("API Endpoint Scanner v0.3\n", style="green bold")
		info_text.append("Github: github.com/jujubooom/Api-Finder", style="blue")
		
		# åˆ›å»ºé¢æ¿
		logo_panel = Panel(
			Align.center(logo),
			title="[yellow bold]ğŸš€ API-Finder ğŸš€[/yellow bold]",
			border_style="cyan",
			padding=(1, 2)
		)
		
		info_panel = Panel(
			Align.center(info_text),
			border_style="green",
			padding=(0, 2)
		)
		
		# æ˜¾ç¤ºlogoå’Œä¿¡æ¯
		console.print(logo_panel)
		console.print(info_panel)
		
	except Exception as e:
		# æ€¥æ•‘æªæ–½ - ä½¿ç”¨ç®€å•çš„Richæ˜¾ç¤º
		console.print(Panel(
			"[cyan bold]Api-Finder v0.3[/cyan bold]\n"
			"[blue]Github: github.com/jujubooom/Api-Finder[/blue]",
			title="ğŸš€ API-Finder ğŸš€",
			border_style="cyan"
		))


# Richèµ‹èƒ½çš„è¾“å‡ºç®¡ç†å™¨ç±»
class OutputManager:
	"""
	ä½¿ç”¨Richåº“é‡æ„çš„OutputManagerç±»ï¼Œæä¾›æ›´ç¾è§‚çš„ç»ˆç«¯è¾“å‡º
	
	silent_mode: é™é»˜æ¨¡å¼ï¼Œåªè¾“å‡ºå‘ç°çš„APIç«¯ç‚¹ (Silent mode, only output discovered API endpoints)
	verbose_mode: è¯¦ç»†è¾“å‡ºæ¨¡å¼ (Verbose output mode)
	output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ (Output file path)
	results: ç»“æœåˆ—è¡¨ (Results list)
	stats: ç»Ÿè®¡ä¿¡æ¯ (Statistics)
	"""
	def __init__(self, silent_mode, verbose_mode=False, output_file=None):
		self.silent_mode = silent_mode
		self.verbose_mode = verbose_mode
		self.output_file = output_file
		self.console = console  # ä½¿ç”¨å…¨å±€çš„Rich console
		self.results = []
		self.stats = {
			"total_urls": 0,
			"successful_requests": 0,
			"failed_requests": 0,
			"api_endpoints": 0
		}
	
	def print_info(self, text):
		if not self.silent_mode:
			self.console.print(text)
	
	def print_verbose(self, text):
		if self.verbose_mode and not self.silent_mode:
			self.console.print(f"[cyan][DEBUG][/cyan] {text}")
	
	def print_url(self, url, source=""):
		if self.silent_mode:
			print(url)  # é™é»˜æ¨¡å¼ä»ç”¨æ™®é€šprint
		else:
			if source:
				self.console.print(f"[green bold]âœ“[/green bold] [blue]{url}[/blue] [dim](discovered from: {source})[/dim]")
			else:
				self.console.print(f"[green bold]âœ“[/green bold] [blue]{url}[/blue]")
		
		# ä¿å­˜ç»“æœ (Save results)
		self.results.append({
			"url": url,
			"source": source,
			"timestamp": datetime.now().isoformat()
		})
		self.stats["api_endpoints"] += 1
	
	def print_error(self, text):
		if not self.silent_mode:
			self.console.print(f"[red bold]âœ—[/red bold] {text}")
	
	def print_warning(self, text):
		if not self.silent_mode:
			self.console.print(f"[yellow bold]âš [/yellow bold] {text}")
	
	def print_success(self, text):
		if not self.silent_mode:
			self.console.print(f"[green bold]âœ“[/green bold] {text}")

	# è¾“å‡ºä½¿ç”¨çš„ä»£ç†æ¨¡å¼ (Output proxy mode used)
	def print_proxy_mode(self, proxies):
		if not self.silent_mode:
			if proxies:
				proxy_table = Table(title="ğŸŒ Proxy Configuration", border_style="blue")
				proxy_table.add_column("Type", style="cyan")
				proxy_table.add_column("Address", style="green")
				
				if isinstance(proxies, list):
					for proxy in proxies:
						proxy_table.add_row("SOCKS5", proxy)
				elif isinstance(proxies, dict):
					for protocol, proxy in proxies.items():
						proxy_table.add_row(protocol.upper(), proxy)
				
				self.console.print(proxy_table)
			else:
				self.console.print("[yellow]ğŸ’» Direct connection (no proxy)[/yellow]")

	def print_stats(self):
		if not self.silent_mode:
			# åˆ›å»ºç»Ÿè®¡è¡¨æ ¼
			stats_table = Table(title="ğŸ“Š Scan Statistics", border_style="cyan")
			stats_table.add_column("Item", style="yellow bold")
			stats_table.add_column("Value", style="green bold", justify="right")
			
			stats_table.add_row("ğŸ¯ Total URLs", str(self.stats['total_urls']))
			stats_table.add_row("âœ… Successful Requests", str(self.stats['successful_requests']))
			stats_table.add_row("âŒ Failed Requests", str(self.stats['failed_requests']))
			stats_table.add_row("ğŸ” API Endpoints Found", str(self.stats['api_endpoints']))
			
			# è®¡ç®—æˆåŠŸç‡
			total_requests = self.stats['successful_requests'] + self.stats['failed_requests']
			if total_requests > 0:
				success_rate = (self.stats['successful_requests'] / total_requests) * 100
				stats_table.add_row("ğŸ“ˆ Success Rate", f"{success_rate:.1f}%")
			
			self.console.print("\n")
			self.console.print(stats_table)
			self.console.print("\n")
	
	def save_results(self):
		if not self.output_file:
			return
		
		try:
			file_ext = os.path.splitext(self.output_file)[1].lower()
			
			if file_ext == '.json':
				with open(self.output_file, 'w', encoding='utf-8') as f:
					json.dump({
						"scan_info": {
							"target_url": arg.url,
							"scan_time": datetime.now().isoformat(),
							"stats": self.stats
						},
						"results": self.results
					}, f, ensure_ascii=False, indent=2)
			
			elif file_ext == '.txt':
				with open(self.output_file, 'w', encoding='utf-8') as f:
					f.write(f"{i18n.get('output_header')}\n")
					f.write(f"{i18n.get('output_target')}: {arg.url}\n")
					f.write(f"{i18n.get('output_scan_time')}: {datetime.now().isoformat()}\n")
					f.write(f"{i18n.get('output_endpoints_found')}: {self.stats['api_endpoints']}\n")
					f.write("-" * 50 + "\n")
					for result in self.results:
						f.write(f"{result['url']}\n")
			
			elif file_ext == '.csv':
				import csv
				with open(self.output_file, 'w', newline='', encoding='utf-8') as f:
					writer = csv.writer(f)
					writer.writerow(['URL', 'Source', 'Timestamp'])
					for result in self.results:
						writer.writerow([result['url'], result['source'], result['timestamp']])
			
			if not self.silent_mode:
				self.console.print(f"[green bold]ğŸ’¾ Results saved to:[/green bold] [blue]{self.output_file}[/blue]")
				
		except Exception as e:
			self.print_error(f"Save failed: {str(e)}")

# åˆå§‹åŒ–è¾“å‡ºç®¡ç†å™¨ (Initialize output manager)
output = OutputManager(arg.silent, arg.verbose, arg.output)
proxies_global = None

def do_proxys():
	global proxies_global
	
	if proxies_global is not None:
		return proxies_global
	
	if arg.proxy == "0":
		# è‡ªåŠ¨è·å–ä»£ç†åˆ—è¡¨ (Auto fetch proxy list)
		header = {"User-Agent": Uam.getUa()}
		proxy_response = requests.get("https://proxy.scdn.io/api/get_proxy.php?protocol=socks5&count=5", headers=header).text
		proxy_data = json.loads(proxy_response)
		if proxy_data.get("code") == 200 and "data" in proxy_data and "proxies" in proxy_data["data"]:
			proxies_global = proxy_data["data"]["proxies"]
		else:
			output.print_error(i18n.get('proxy_fetch_failed'))
			proxies_global = []

	elif arg.proxy:
		# åˆ¤æ–­ä»£ç†ç±»å‹æ˜¯å¦ä¸ºsocks5
		if arg.proxy.startswith('socks5://'):
			proxies_global = {
				"http": arg.proxy,
				"https": arg.proxy
			}
		# æ™®é€šhttp/httpsä»£ç†
		else:
			proxies_global = {
				"http": arg.proxy if arg.proxy.startswith('http') else f'http://{arg.proxy}',
				"https": arg.proxy if arg.proxy.startswith('http') else f'http://{arg.proxy}'
			}
	
	return proxies_global

# åˆ›å»ºçº¿ç¨‹å®‰å…¨çš„ç»“æœå­˜å‚¨ç»“æ„ (Create thread-safe result storage structure)
class ResultStore:
	def __init__(self):
		self.results = {"GET": {}, "POST": {}}
		self.lock = threading.Lock()

	def update(self, method, success, response_text, error=None):
		with self.lock:
			self.results[method] = {
				"success": success,
				"response": response_text,
				"error": error
			}


# è¯·æ±‚æ‰§è¡Œå‡½æ•° (Request execution function)
def make_request(method, url, cookies, timeout, store):
	# è¯·æ±‚å‰çš„é…ç½® (Request configuration)
	proxies = do_proxys()
	if proxies and isinstance(proxies, list):
		proxies = {
			"socks5": proxies[random.randint(0,len(proxies)-1)],
		}
	header = {"User-Agent": Uam.getUa()}

	try:
		if method == "GET":
			if proxies:
				res = requests.get(url, headers=header, cookies=cookies,
								   timeout=timeout, proxies=proxies)
			else:
				res = requests.get(url, headers=header, cookies=cookies,
								   timeout=timeout)
		else:  # POST
			if proxies:
				res = requests.post(url, headers=header, cookies=cookies,
								   timeout=timeout, proxies=proxies)
			else:
				res = requests.post(url, headers=header, cookies=cookies,
								   timeout=timeout)

		res.raise_for_status()
		response_text = res.text.replace(" ", "").replace("\n", "")
		store.update(method, True, response_text)

	except requests.exceptions.RequestException as e:
		store.update(method, False, None, str(e))
	except Exception as e:
		store.update(method, False, None, str(e))


def do_request(url):
	result_store = ResultStore()

	# åˆ›å»ºå¹¶å¯åŠ¨çº¿ç¨‹
	get_thread = threading.Thread(
		target=make_request,
		args=("GET", url, arg.cookie, arg.timeout, result_store)
	)

	post_thread = threading.Thread(
		target=make_request,
		args=("POST", url, arg.cookie, arg.timeout, result_store)
	)

	# å¯åŠ¨çº¿ç¨‹
	get_thread.start()
	post_thread.start()

	# ç­‰å¾…ä¸¤ä¸ªçº¿ç¨‹å®Œæˆ
	get_thread.join()
	post_thread.join()

	# ç»Ÿä¸€è¾“å‡ºç»“æœ (Unified output results)
	for method in ["GET", "POST"]:
		result = result_store.results[method]
		if result["success"]:
			if method == "GET" and output.silent_mode:
				print(url)
			elif not output.silent_mode:
				output.print_success(f"{method} request successful")
				if output.verbose_mode:
					res_len = len(result["response"])
					output.print_verbose(f"ğŸ“ Response length: {res_len} characters")
					output.print_verbose(f"ğŸ‘€ Response preview: {result['response'][:200]}...")

			output.stats["successful_requests"] += 1
		else:
			output.print_error(f"{method} request failed: {result['error']}")
			output.stats["failed_requests"] += 1
	# è¯·æ±‚é—´éš”
	time.sleep(arg.delay)

def find_last(string,str):
	positions = []
	last_position=-1
	while True:
		position = string.find(str,last_position+1)
		if position == -1:break
		last_position = position
		positions.append(position)
	return positions

# Handling relative URLs
# åˆ é™¤åŸæœ‰çš„ process_url å’Œ extract_URL å‡½æ•°å®šä¹‰

# è·å–HTMLå†…å®¹ (Extract HTML content)
def Extract_html(URL):
	"""
	URL: ç›®æ ‡URL (Target URL)
	header: è¯·æ±‚å¤´ (Request headers)
	raw: è¯·æ±‚è¿”å›çš„å†…å®¹ (Raw response content)
	content: è§£æåçš„HTMLå†…å®¹ (Parsed HTML content)
	return: è¿”å›HTMLå†…å®¹ (Return HTML content)
	"""
	header = {"User-Agent": Uam.getUa()}
	try:
		raw = requests.get(URL, headers=header, timeout=arg.timeout, cookies=arg.cookie)
		raw.raise_for_status()
		content = raw.content.decode("utf-8", "ignore")
		output.print_verbose(f"âœ… Successfully retrieved HTML content: {URL}")
		return content
	except requests.exceptions.RequestException as e:
		output.print_error(f"Failed to get HTML {URL}: {str(e)}")
		return None
	except Exception as e:
		output.print_error(f"HTML extraction exception {URL}: {str(e)}")
		return None


def find_by_url(url):
	try:
		output.print_info(f"ğŸ¯ [bold blue]Starting scan target:[/bold blue] [green]{url}[/green]")
	except:
		output.print_info("âŒ Please specify a valid URL, e.g.: https://www.baidu.com")
		return None
	
	html_raw = Extract_html(url)
	if html_raw == None: 
		output.print_error(f"Cannot access {url}")
		return None
	
	output.print_verbose("ğŸ” Starting to parse HTML content...")
	html = BeautifulSoup(html_raw, "html.parser")
	html_scripts = html.findAll("script")
	output.print_verbose(f"ğŸ“„ Found {len(html_scripts)} script tags")
	
	script_array = {}
	script_temp = ""
	
	for html_script in html_scripts:
		script_src = html_script.get("src")
		if script_src == None:
			script_temp += html_script.get_text() + "\n"
		else:
			purl = URLProcessor.process_url(url, script_src)
			script_content = Extract_html(purl)
			if script_content:
				script_array[purl] = script_content
			else:
				output.print_warning(f"Cannot get external script: {purl}")
	
	script_array[url] = script_temp
	
	allurls = {}
	for script in script_array:
		output.print_verbose(f"ğŸ” Analyzing script: {script}")
		temp_urls = URLExtractor.extract_urls(script_array[script])
		if len(temp_urls) == 0: 
			output.print_verbose("ğŸ” No URLs found")
			continue
		output.print_verbose(f"âœ… Found {len(temp_urls)} URLs")
		for temp_url in temp_urls:
			allurls[script] = temp_urls
	result_store = ResultStore()

	for i in allurls:
		for j in allurls[i]:
			output.print_url(j, i)
			temp1 = urlparse(j)
			temp2 = urlparse(url)
			
			if temp1.netloc != urlparse("1").netloc:
				do_request(j)
			else:
				do_request(temp2.scheme+"://"+temp2.netloc+j)



# è®¾ç½®ä¸€ä¸ªä¸»å‡½æ•°ï¼Œæ–¹ä¾¿åç»­æ·»åŠ æ–°çš„åŠŸèƒ½
def main():
	try:
		# é™¤äº†é™é»˜æ¨¡å¼ï¼Œå…¶ä»–æƒ…å†µä¸‹æ˜¾ç¤ºé¡¹ç›®logo
		if not arg.silent:
			show_logo()
		
		# æ˜¾ç¤ºä»£ç†æ¨¡å¼
		output.print_proxy_mode(do_proxys())

		results = find_by_url(arg.url)
		# æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
		output.print_stats()
		
		# ä¿å­˜ç»“æœ
		output.save_results()

	# å¤„ç†ä¸­é€”é€€å‡ºæƒ…å†µï¼Œé˜²æ­¢è¾“å‡ºä¸€å †æŠ¥é”™
	except KeyboardInterrupt:
		output.print_warning("ğŸ›‘ User interrupted scan")
		output.print_stats()
		output.save_results()
	except Exception as e:
		output.print_error(f"Program execution exception: {str(e)}")
		sys.exit(1)

if __name__ == "__main__":
	main()