#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å·¥å…·ç±» (Utility Classes)
åŒ…å«Api-Finderä¸­ä½¿ç”¨çš„é€šç”¨åŠŸèƒ½ (Contains common functionality used in Api-Finder)
"""

import re
import yaml
import requests
from datetime import datetime, timedelta
from urllib.parse import urlparse
from .config import DEFAULT_CONFIG

def load_rules():
    """ä» rules.yaml åŠ è½½è§„åˆ™"""
    import os
    rules_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config', 'rules.yaml')
    with open(rules_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

RULES = load_rules()

class URLProcessor:
    """URLå¤„ç†å·¥å…·ç±» (URL processing utility class)"""
    
    @staticmethod
    def process_url(base_url, relative_url):
        """
        å¤„ç†ç›¸å¯¹URLï¼Œè½¬æ¢ä¸ºç»å¯¹URL (Process relative URL, convert to absolute URL)
        
        Args:
            base_url (str): åŸºç¡€URL (Base URL)
            relative_url (str): ç›¸å¯¹URL (Relative URL)
            
        Returns:
            str: ç»å¯¹URL (Absolute URL)
        """
        black_url = ["javascript:"]
        url_raw = urlparse(base_url)
        ab_url = url_raw.netloc
        host_url = url_raw.scheme
        
        if relative_url[0:2] == "//":
            result = host_url + ":" + relative_url
        elif relative_url[0:4] == "http":
            result = relative_url
        elif relative_url[0:2] != "//" and relative_url not in black_url:
            if relative_url[0:1] == "/":
                result = host_url + "://" + ab_url + relative_url
            else:
                if relative_url[0:1] == ".":
                    if relative_url[0:2] == "..":
                        result = host_url + "://" + ab_url + relative_url[2:]
                    else:
                        result = host_url + "://" + ab_url + relative_url[1:]
                else:
                    result = host_url + "://" + ab_url + "/" + relative_url
        else:
            result = base_url
        return result

class URLExtractor:
    """URLæå–å·¥å…·ç±» (URL extraction utility class)"""
    
    @staticmethod
    def extract_urls_from_html(html_content):
        """
        ä»HTMLå†…å®¹ä¸­æå–URL (Extract URLs from HTML content)
        
        Args:
            html_content (str): HTMLå†…å®¹ (HTML content)
            
        Returns:
            list: æå–åˆ°çš„URLåˆ—è¡¨ (List of extracted URLs)
        """
        from bs4 import BeautifulSoup
        filter_key = DEFAULT_CONFIG["filter_extensions"]
        ignored_domains = RULES.get('ignored_domains', [])
        
        urls = []
        
        try:
            soup = BeautifulSoup(html_content, "html.parser")
            
            # å®šä¹‰è¦æŸ¥æ‰¾çš„HTMLå±æ€§
            url_attributes = [
                ('a', 'href'),
                ('link', 'href'),
                ('img', 'src'),
                ('script', 'src'),
                ('iframe', 'src'),
                ('form', 'action'),
                ('area', 'href'),
                ('source', 'src'),
                ('track', 'src'),
                ('audio', 'src'),
                ('video', 'src'),
                ('embed', 'src'),
                ('object', 'data'),
                ('frame', 'src'),
                ('meta', 'content'),
                ('base', 'href'),
            ]
            
            # æå–æ‰€æœ‰å¯èƒ½çš„URL
            for tag_name, attr_name in url_attributes:
                tags = soup.find_all(tag_name)
                for tag in tags:
                    url = tag.get(attr_name)
                    if url and isinstance(url, str):
                        # æ¸…ç†URL
                        url = url.strip().strip('"').strip("'")
                        
                        # è¿‡æ»¤æ‰æ— æ•ˆçš„URL
                        if not url or url.startswith('#') or url.startswith('javascript:') or url.startswith('mailto:') or url.startswith('tel:'):
                            continue
                        
                        # è¿‡æ»¤æ‰ä¸éœ€è¦çš„æ–‡ä»¶æ‰©å±•å
                        if any(ext in url.lower() for ext in filter_key):
                            continue
                            
                        # è¿‡æ»¤æ‰è¢«å¿½ç•¥çš„åŸŸå
                        if any(domain in url.lower() for domain in ignored_domains):
                            continue
                        
                        # åªä¿ç•™ç›¸å¯¹è·¯å¾„æˆ–APIç›¸å…³çš„URL
                        if (url.startswith('/') or 
                            url.startswith('./') or 
                            url.startswith('../') or
                            'api' in url.lower() or
                            'ajax' in url.lower() or
                            'json' in url.lower() or
                            'xml' in url.lower() or
                            url.endswith('.php') or
                            url.endswith('.jsp') or
                            url.endswith('.asp') or
                            url.endswith('.aspx') or
                            url.endswith('.action') or
                            url.endswith('.do') or
                            url.endswith('.json') or
                            url.endswith('.xml') or
                            url.endswith('.txt') or
                            url.endswith('.html') or
                            url.endswith('.htm')):
                            
                            if url not in urls:
                                urls.append(url)
            
            # ä¹ŸæŸ¥æ‰¾dataå±æ€§ä¸­çš„URL
            all_tags = soup.find_all()
            for tag in all_tags:
                if tag.attrs:
                    for attr, value in tag.attrs.items():
                        if attr.startswith('data-') and isinstance(value, str):
                            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾URLæ¨¡å¼
                            import re
                            url_pattern = r'["\']([^"\']+(?:\.php|\.jsp|\.asp|\.aspx|\.action|\.do|\.json|\.xml|/api/|/ajax/)[^"\']*)["\']'
                            matches = re.findall(url_pattern, value)
                            for match in matches:
                                if match not in urls:
                                    urls.append(match)
            
        except Exception as e:
            # å¦‚æœHTMLè§£æå¤±è´¥ï¼Œè¿”å›ç©ºåˆ—è¡¨
            pass
            
        return urls
    
    @staticmethod
    def extract_urls(js_content):
        """
        ä»JavaScriptå†…å®¹ä¸­æå–URL (Extract URLs from JavaScript content)
        
        Args:
            js_content (str): JavaScriptå†…å®¹ (JavaScript content)
            
        Returns:
            list: æå–åˆ°çš„URLåˆ—è¡¨ (List of extracted URLs)
        """
        filter_key = DEFAULT_CONFIG["filter_extensions"]
        pattern_raw = RULES.get('url_extractor_pattern', '')
        ignored_domains = RULES.get('ignored_domains', [])

        pattern = re.compile(pattern_raw, re.VERBOSE)
        result = re.finditer(pattern, str(js_content))
        urls = []
        
        if result is None:
            return urls
            
        for match in result:
            url = match.group().strip('"').strip("'")
            if any(sub in url for sub in filter_key):
                continue
            if any(domain in url for domain in ignored_domains):
                continue
            
            urls.append(url)
        
        return urls

class UpdateManager:
    """æ›´æ–°ç®¡ç†å·¥å…·ç±»"""

    @staticmethod
    def get_current_timestamp():
        """è·å–å½“å‰æ—¶é—´çš„ YYYYMMDDHHMMSS æ ¼å¼æ—¶é—´æˆ³"""
        return datetime.now().strftime('%Y%m%d%H%M%S')

    @staticmethod
    def check_for_updates(force_update=False):
        """
        æ£€æŸ¥å¹¶æ‰§è¡Œè§„åˆ™æ–‡ä»¶æ›´æ–°, ä¼šåˆå¹¶ç”¨æˆ·è‡ªå®šä¹‰çš„åˆ—è¡¨è§„åˆ™ã€‚
        
        Args:
            force_update (bool): æ˜¯å¦å¼ºåˆ¶æ›´æ–°
        """
        local_rules = RULES
        last_check_str = str(local_rules.get('last_check_timestamp', '20000101000000'))
        last_check_time = datetime.strptime(last_check_str, '%Y%m%d%H%M%S')
        update_interval = timedelta(days=DEFAULT_CONFIG['update_interval_days'])

        if not force_update and (datetime.now() - last_check_time < update_interval):
            return

        from rich.console import Console
        from rich.panel import Panel
        console = Console()
        
        try:
            remote_url = DEFAULT_CONFIG['remote_rules_url']
            response = requests.get(remote_url, timeout=DEFAULT_CONFIG['timeout'])
            response.raise_for_status()
            
            remote_rules = yaml.safe_load(response.text)
            local_version = str(local_rules.get('version_timestamp', '0'))
            remote_version = str(remote_rules.get('version_timestamp', '0'))
            
            rules_updated = False
            if force_update or remote_version > local_version:
                if force_update:
                    console.print(Panel("ğŸ”„ [bold yellow]å¼ºåˆ¶æ›´æ–°è§„åˆ™...[/bold yellow]", border_style="yellow"))
                else:
                    console.print(Panel(f"ğŸ†• [bold green]å‘ç°æ–°ç‰ˆæœ¬è§„åˆ™ (v{remote_version})ï¼Œæ­£åœ¨åˆå¹¶è§„åˆ™...[/bold green]", border_style="green"))

                # --- åˆå¹¶é€»è¾‘ ---
                # ä»¥è¿œç¨‹è§„åˆ™ä¸ºåŸºç¡€è¿›è¡Œåˆå¹¶
                merged_rules = remote_rules.copy()

                # åˆå¹¶æ‰€æœ‰åˆ—è¡¨ç±»å‹çš„å€¼
                for key, local_value in local_rules.items():
                    if isinstance(local_value, list):
                        remote_value = merged_rules.get(key, [])
                        if isinstance(remote_value, list):
                            # åˆå¹¶æœ¬åœ°å’Œè¿œç¨‹åˆ—è¡¨å¹¶å»é‡
                            merged_list = sorted(list(set(local_value + remote_value)))
                            merged_rules[key] = merged_list
                
                # æ›´æ–°æœ€åæ£€æŸ¥æ—¶é—´æˆ³
                merged_rules['last_check_timestamp'] = UpdateManager.get_current_timestamp()

                import os
                rules_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config', 'rules.yaml')
                with open(rules_path, 'w', encoding='utf-8') as f:
                    yaml.dump(merged_rules, f, allow_unicode=True, sort_keys=False)
                
                console.print("âœ… [bold green]è§„åˆ™æ–‡ä»¶æ›´æ–°å¹¶åˆå¹¶æˆåŠŸã€‚[/bold green]")
                rules_updated = True

            else:
                console.print("â„¹ï¸ [bold cyan]æœ¬åœ°è§„åˆ™å·²æ˜¯æœ€æ–°ç‰ˆæœ¬ã€‚[/bold cyan]")

            # å¦‚æœè§„åˆ™æ²¡æœ‰æ›´æ–°ï¼Œä»…æ›´æ–°æ£€æŸ¥æ—¶é—´æˆ³
            if not rules_updated:
                local_rules['last_check_timestamp'] = UpdateManager.get_current_timestamp()
                import os
                rules_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'config', 'rules.yaml')
                with open(rules_path, 'w', encoding='utf-8') as f:
                    yaml.dump(local_rules, f, allow_unicode=True, sort_keys=False)

        except requests.RequestException as e:
            console.print(f"âŒ [bold red]æ£€æŸ¥æ›´æ–°å¤±è´¥:[/bold red] {e}")
        except yaml.YAMLError as e:
            console.print(f"âŒ [bold red]è§£æè¿œç¨‹æˆ–æœ¬åœ°è§„åˆ™æ–‡ä»¶å¤±è´¥:[/bold red] {e}")
        except Exception as e:
            console.print(f"âŒ [bold red]æ›´æ–°è¿‡ç¨‹ä¸­å‘ç”ŸæœªçŸ¥é”™è¯¯:[/bold red] {e}")

        