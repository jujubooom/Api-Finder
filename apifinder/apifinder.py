"""
@date: 2025
@version: 0.3.1
@description: ç”¨äºæ‰«æAPIç«¯ç‚¹
"""

import random
import requests
from requests.adapters import HTTPAdapter
from urllib.parse import urlparse
from bs4 import BeautifulSoup
import argparse
import time
import sys
import json
import os
from datetime import datetime
from urllib3.exceptions import InsecureRequestWarning
import urllib3
from .ua_manager import UaManager
from .utils import URLProcessor, URLExtractor, UpdateManager
from .i18n import i18n
from .output_manager import OutputManager, FileOutputManager
import threading
import pyfiglet
from rich.console import Console
from rich.text import Text
from rich.panel import Panel
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.align import Align
from rich.live import Live
from rich.status import Status
from rich.json import JSON
from rich.traceback import install
from rich.columns import Columns
from rich.rule import Rule
from concurrent.futures import ThreadPoolExecutor, as_completed

# ç¦ç”¨SSLè­¦å‘Š
urllib3.disable_warnings(InsecureRequestWarning)

# å®‰è£…Richçš„å¼‚å¸¸å¤„ç†
install()

parser = argparse.ArgumentParser(description="Api-Finder v0.3")
parser.add_argument("-u", "--url", help=i18n.get('arg_url_help'), required=True)
parser.add_argument("-c", "--cookie", help=i18n.get('arg_cookie_help'))
parser.add_argument("-p", "--proxy", help=i18n.get('arg_proxy_help'))
parser.add_argument("-s", "--silent", action="store_true", help=i18n.get('arg_silent_help'))
parser.add_argument("-o", "--output", help=i18n.get('arg_output_help'))
parser.add_argument("-t", "--timeout", type=int, default=10, help=i18n.get('arg_timeout_help'))
parser.add_argument("-T", "--threads", type=int, default=10, help=i18n.get('arg_threads_help'))
parser.add_argument("-d", "--delay", type=float, default=0.5, help=i18n.get('arg_delay_help'))
parser.add_argument("-v", "--verbose", action="store_true", help=i18n.get('arg_verbose_help'))
parser.add_argument("-r", "--random", action="store_true", help=i18n.get('arg_random_help'))
parser.add_argument("-a", "--app", help=i18n.get('arg_app_help'), default='common')
parser.add_argument("-U", "--update", action="store_true", help=i18n.get('arg_update_help'))
parser.add_argument("-D", "--depth", type=int, default=2, help=i18n.get('arg_depth_help'))


arg = parser.parse_args()

# åˆå§‹åŒ–Rich Console (Initialize Rich Console)
console = Console()

# åˆå§‹åŒ–UAç®¡ç†å™¨ (Initialize UA Manager)
Uam = UaManager(arg.app, arg.random)

# ä½¿ç”¨Riché‡æ„çš„Logoæ˜¾ç¤º
def show_logo():
	"""ä½¿ç”¨Richå’Œpyfigletæ˜¾ç¤ºç²¾ç¾logo"""
	try:
		# ç”ŸæˆASCII art
		logo_text = pyfiglet.figlet_format("Api-Finder", font="slant")
		
		# åˆ›å»ºå¸¦é¢œè‰²çš„logoæ–‡æœ¬
		logo = Text(logo_text, style="cyan bold")
		
		# åˆ›å»ºé¡¹ç›®ä¿¡æ¯æ–‡æœ¬
		info_text = Text()
		info_text.append("API Endpoint Scanner v0.5", style="green bold")
		info_text.append("     Github: github.com/jujubooom/Api-Finder\n", style="blue")
		info_text.append("Developed by jujubooom,bx,orxiain", style="green bold")
		
		# åˆ›å»ºé¢æ¿
		logo_panel = Panel(
			Align.center(logo),
			title="[yellow bold]ğŸš€ API-Finder ğŸš€[/yellow bold]",
			border_style="cyan",
			padding=(1, 2)
		)
		
		info_panel = Panel(
			Align.center(info_text),
			border_style="green",
			padding=(0, 2)
		)
		
		# æ˜¾ç¤ºlogoå’Œä¿¡æ¯
		console.print(logo_panel)
		console.print(info_panel)
		console.print(Rule(style="dim"))
		
	except Exception as e:
		# æ€¥æ•‘æªæ–½ - ä½¿ç”¨ç®€å•çš„Richæ˜¾ç¤º
		console.print(Panel(
			"[cyan bold]Api-Finder v0.3[/cyan bold]\n"
			"[blue]Github: github.com/jujubooom/Api-Finder[/blue]",
			title="ğŸš€ API-Finder ğŸš€",
			border_style="cyan"
		))



# åˆå§‹åŒ–è¾“å‡ºç®¡ç†å™¨ (Initialize output manager)
output = OutputManager(arg.silent, arg.verbose, arg.output)
file_output = FileOutputManager(output)
proxies_global = None

def do_proxys():
	global proxies_global
	
	if proxies_global is not None:
		return proxies_global
	
	if arg.proxy == "0":
		# è‡ªåŠ¨è·å–ä»£ç†åˆ—è¡¨ (Auto fetch proxy list)
		header = {"User-Agent": Uam.getUa()}
		proxy_response = requests.get("https://proxy.scdn.io/api/get_proxy.php?protocol=socks5&count=5", headers=header).text
		proxy_data = json.loads(proxy_response)
		if proxy_data.get("code") == 200 and "data" in proxy_data and "proxies" in proxy_data["data"]:
			proxies_global = proxy_data["data"]["proxies"]
		else:
			output.print_error(i18n.get('proxy_fetch_failed'))
			proxies_global = []

	elif arg.proxy:
		# åˆ¤æ–­ä»£ç†ç±»å‹æ˜¯å¦ä¸ºsocks5
		if arg.proxy.startswith('socks5://'):
			proxies_global = {
				"http": arg.proxy,
				"https": arg.proxy
			}
		# æ™®é€šhttp/httpsä»£ç†
		else:
			proxies_global = {
				"http": arg.proxy if arg.proxy.startswith('http') else f'http://{arg.proxy}',
				"https": arg.proxy if arg.proxy.startswith('http') else f'http://{arg.proxy}'
			}
	
	return proxies_global

# åˆ›å»ºçº¿ç¨‹å®‰å…¨çš„ç»“æœå­˜å‚¨ç»“æ„ (Create thread-safe result storage structure)
class ResultStore:
	def __init__(self):
		self.results = {"GET": {}, "POST": {}}
		self.lock = threading.Lock()

	def update(self, method, success, response_text, error=None):
		with self.lock:
			self.results[method] = {
				"success": success,
				"response": response_text,
				"error": error
			}


# æ·±åº¦æ‰«æ
class DeepScanManager:
	def __init__(self, base_url, max_depth=2):
		self.base_url = base_url
		self.base_domain = urlparse(base_url).netloc
		self.max_depth = max_depth
		self.scanned_urls = set()  # å·²æ‰«æçš„URLé›†åˆ
		self.lock = threading.Lock()
	
	def is_same_domain(self, url):
		try:
			parsed_url = urlparse(url)
			return parsed_url.netloc == self.base_domain
		except:
			return False
	
	def add_scanned_url(self, url):
		with self.lock:
			self.scanned_urls.add(url)
	
	def is_already_scanned(self, url):
		with self.lock:
			return url in self.scanned_urls
	
	def get_filtered_urls(self, urls):
		filtered_urls = []
		for url in urls:
			if self.is_same_domain(url) and not self.is_already_scanned(url):
				filtered_urls.append(url)
		return filtered_urls


# è¯·æ±‚æ‰§è¡Œå‡½æ•° (Request execution function)
def make_request(method, url, cookies, timeout, store):
	# è¯·æ±‚å‰çš„é…ç½® (Request configuration)
	proxies = do_proxys()
	if proxies and isinstance(proxies, list):
		proxies = {
			"socks5": proxies[random.randint(0,len(proxies)-1)],
		}
	
	# æ›´å®Œæ•´çš„è¯·æ±‚å¤´
	header = {
		"User-Agent": Uam.getUa(),
		"Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
		"Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
		"Accept-Encoding": "gzip, deflate, br",
		"Connection": "keep-alive",
		"Upgrade-Insecure-Requests": "1",
		"Cache-Control": "max-age=0"
	}
	
	max_retries = 2
	retry_delay = 0.5
	
	for attempt in range(max_retries):
		try:
			session = requests.Session()
			session.verify = False  # ç¦ç”¨SSLéªŒè¯
			
			adapter = HTTPAdapter(max_retries=1)
			session.mount('http://', adapter)
			session.mount('https://', adapter)
			if proxies:
				session.proxies.update(proxies)
			
			if method == "GET":
				res = session.get(
					url, 
					headers=header, 
					cookies=cookies, 
					timeout=(5, timeout),
					allow_redirects=True
				)
			else:  # POST
				res = session.post(
					url, 
					headers=header, 
					cookies=cookies, 
					timeout=(5, timeout),
					allow_redirects=True
				)

			if res.status_code in [301, 302, 303, 307, 308]:
				redirect_url = res.url
				if redirect_url != url:
					output.print_verbose(f"ğŸ”„ Redirect detected in {method} request: {url} -> {redirect_url}")
					return make_request(method, redirect_url, cookies, timeout, store)

			res.raise_for_status()
			
			if res.encoding is None or res.encoding == 'ISO-8859-1':
				res.encoding = 'utf-8'
			
			original_response_text = res.text
			response_text = res.text.replace(" ", "").replace("\n", "")
			
			store.update(method, True, original_response_text)
			return
			
		except requests.exceptions.SSLError as e:
			if attempt < max_retries - 1:
				time.sleep(retry_delay)
				retry_delay *= 2
				continue
			else:
				store.update(method, False, None, f"SSL error: {str(e)}")
				return
				
		except requests.exceptions.ConnectionError as e:
			if attempt < max_retries - 1:
				time.sleep(retry_delay)
				retry_delay *= 2
				continue
			else:
				store.update(method, False, None, f"Connection error: {str(e)}")
				return
				
		except requests.exceptions.Timeout as e:
			if attempt < max_retries - 1:
				time.sleep(retry_delay)
				retry_delay *= 2
				continue
			else:
				store.update(method, False, None, f"Timeout: {str(e)}")
				return
				
		except requests.exceptions.RequestException as e:
			if attempt < max_retries - 1:
				time.sleep(retry_delay)
				retry_delay *= 2
				continue
			else:
				store.update(method, False, None, f"Request error: {str(e)}")
				return
				
		except Exception as e:
			if attempt < max_retries - 1:
				time.sleep(retry_delay)
				retry_delay *= 2
				continue
			else:
				store.update(method, False, None, f"Unexpected error: {str(e)}")
				return


def do_request(url):
	result_store = ResultStore()

	# åˆ›å»ºå¹¶å¯åŠ¨çº¿ç¨‹
	get_thread = threading.Thread(
		target=make_request,
		args=("GET", url, {"Cookie": arg.cookie}, arg.timeout, result_store)
	)

	post_thread = threading.Thread(
		target=make_request,
		args=("POST", url, {"Cookie": arg.cookie}, arg.timeout, result_store)
	)

	# å¯åŠ¨çº¿ç¨‹
	get_thread.start()
	post_thread.start()

	# ç­‰å¾…ä¸¤ä¸ªçº¿ç¨‹å®Œæˆ
	get_thread.join()
	post_thread.join()
	
	response_text_to_return = None

	# ç»Ÿä¸€è¾“å‡ºç»“æœ (Unified output results)
	for method in ["GET", "POST"]:
		result = result_store.results[method]
		if result["success"]:
			response_text = result['response']
			
			if method == "GET":
				response_text_to_return = response_text
				# å°è¯•è§£æå’Œæ‰“å°æ ‡é¢˜
				try:
					if response_text and '<html' in response_text.lower():
						soup = BeautifulSoup(response_text, 'html.parser')
						if soup.title and soup.title.string:
							title = soup.title.string.strip().replace('\\n', '').replace('\\r', '')
							if title:
								output.print_title(url, title)
				except Exception as e:
					output.print_verbose(f"Could not parse title from {url}: {e}")

			if method == "GET" and output.silent_mode:
				output.console.print(url, highlight=False)
			elif not output.silent_mode:
				output.print_success(f"{method} request successful for {url}")
				if output.verbose_mode:
					res_len = len(response_text)
					output.print_verbose(f"ğŸ“ Response length: {res_len} characters")
					output.print_verbose(f"ğŸ‘€ Response preview: {response_text[:200]}...")

			output.stats["successful_requests"] += 1
		else:
			# åªæœ‰GETè¯·æ±‚å¤±è´¥æ—¶æ‰è¾“å‡ºé”™è¯¯ä¿¡æ¯ï¼ŒPOSTè¯·æ±‚å¤±è´¥æ—¶ä¸è¾“å‡º
			if method == "GET":
				output.print_error(f"{method} request failed for {url}: {result['error']}")
			output.stats["failed_requests"] += 1
	
	# è¯·æ±‚é—´éš”
	time.sleep(arg.delay)
	return response_text_to_return


# è·å–HTMLå†…å®¹ (Extract HTML content)
def Extract_html(URL, follow_redirects=True):
	"""
	URL: ç›®æ ‡URL (Target URL)
	header: è¯·æ±‚å¤´ (Request headers)
	raw: è¯·æ±‚è¿”å›çš„å†…å®¹ (Raw response content)
	content: è§£æåçš„HTMLå†…å®¹ (Parsed HTML content)
	return: è¿”å›HTMLå†…å®¹ (Return HTML content)
	"""
	# æ›´å®Œæ•´çš„è¯·æ±‚å¤´
	header = {
		"User-Agent": Uam.getUa(),
		"Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
		"Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
		"Accept-Encoding": "gzip, deflate, br",
		"Connection": "keep-alive",
		"Upgrade-Insecure-Requests": "1",
		"Sec-Fetch-Dest": "document",
		"Sec-Fetch-Mode": "navigate",
		"Sec-Fetch-Site": "none",
		"Cache-Control": "max-age=0"
	}
	
	# è®¾ç½®é‡è¯•æ¬¡æ•°
	max_retries = 3
	retry_delay = 1
	
	for attempt in range(max_retries):
		try:
			# é…ç½®sessionä»¥å¤„ç†SSLå’Œè¿æ¥é—®é¢˜
			session = requests.Session()
			session.verify = False  # ç¦ç”¨SSLéªŒè¯
			
			# è®¾ç½®é€‚é…å™¨ä»¥å¤„ç†é‡è¯•
			adapter = HTTPAdapter(max_retries=2)
			session.mount('http://', adapter)
			session.mount('https://', adapter)
			
			# æ·»åŠ ä»£ç†æ”¯æŒ
			proxies = do_proxys()
			if proxies and isinstance(proxies, dict):
				session.proxies.update(proxies)
			
			# å‘é€è¯·æ±‚
			raw = session.get(
				URL, 
				headers=header, 
				timeout=(10, 30),  # è¿æ¥è¶…æ—¶10ç§’ï¼Œè¯»å–è¶…æ—¶30ç§’
				cookies=arg.cookie if arg.cookie else None,
				allow_redirects=follow_redirects,  # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦è·Ÿéšé‡å®šå‘
				stream=False
			)
			
			# æ£€æŸ¥é‡å®šå‘çŠ¶æ€ç 
			if follow_redirects and raw.status_code in [301, 302, 303, 307, 308]:
				# è·å–é‡å®šå‘åçš„URL
				redirect_url = raw.url
				if redirect_url != URL:
					output.print_verbose(f"ğŸ”„ Redirect detected: {URL} -> {redirect_url}")
					output.print_info(f"ğŸ“¡ [bold yellow]Following redirect:[/bold yellow] [green]{redirect_url}[/green]")
					# é€’å½’è°ƒç”¨è‡ªèº«è·å–é‡å®šå‘åçš„å†…å®¹
					return Extract_html(redirect_url, follow_redirects=True)
			
			raw.raise_for_status()
			
			# è¿™é‡Œåšäº†ä¸‰ä¸ªå°è¯•ï¼Œå¦‚æœéƒ½å¤±è´¥ï¼Œåˆ™è¿”å›None
			try:
				content = raw.content.decode("utf-8", "ignore")
			except UnicodeDecodeError:
				try:
					content = raw.content.decode("gbk", "ignore")
				except UnicodeDecodeError:
					content = raw.content.decode("latin-1", "ignore")
			
			output.print_verbose(f"âœ… Successfully retrieved HTML content: {URL}")
			return content
			
		except requests.exceptions.SSLError as e:
			if attempt < max_retries - 1:
				output.print_verbose(f"ğŸ”„ SSL error on attempt {attempt + 1}, retrying: {URL}")
				time.sleep(retry_delay)
				retry_delay *= 2
				continue
			else:
				output.print_error(f"SSL error after {max_retries} attempts {URL}: {str(e)}")
				return None
				
		except requests.exceptions.ConnectionError as e:
			if attempt < max_retries - 1:
				output.print_verbose(f"ğŸ”„ Connection error on attempt {attempt + 1}, retrying: {URL}")
				time.sleep(retry_delay)
				retry_delay *= 2
				continue
			else:
				output.print_error(f"Connection error after {max_retries} attempts {URL}: {str(e)}")
				return None
				
		except requests.exceptions.Timeout as e:
			if attempt < max_retries - 1:
				output.print_verbose(f"ğŸ”„ Timeout on attempt {attempt + 1}, retrying: {URL}")
				time.sleep(retry_delay)
				retry_delay *= 2
				continue
			else:
				output.print_error(f"Timeout after {max_retries} attempts {URL}: {str(e)}")
				return None
				
		except requests.exceptions.RequestException as e:
			if attempt < max_retries - 1:
				output.print_verbose(f"ğŸ”„ Request error on attempt {attempt + 1}, retrying: {URL}")
				time.sleep(retry_delay)
				retry_delay *= 2
				continue
			else:
				output.print_error(f"Request failed after {max_retries} attempts {URL}: {str(e)}")
				return None
				
		except Exception as e:
			if attempt < max_retries - 1:
				output.print_verbose(f"ğŸ”„ Unexpected error on attempt {attempt + 1}, retrying: {URL}")
				time.sleep(retry_delay)
				retry_delay *= 2
				continue
			else:
				output.print_error(f"Unexpected error after {max_retries} attempts {URL}: {str(e)}")
				return None
	
	return None


def find_by_url(url, depth=0, deep_scan_manager=None):

	if deep_scan_manager is None:
		deep_scan_manager = DeepScanManager(url, arg.depth)
	
	if depth > deep_scan_manager.max_depth:
		return None
	
	if deep_scan_manager.is_already_scanned(url):
		return None
	
	deep_scan_manager.add_scanned_url(url)
	
	try:
		if depth == 0:
			output.print_info(f"ğŸ¯ [bold blue]Starting scan target:[/bold blue] [green]{url}[/green]")
		else:
			output.print_info(f"ğŸ” [bold blue]Deep scan (depth {depth}):[/bold blue] [green]{url}[/green]")
	except:
		output.print_info("âŒ Please specify a valid URL, e.g.: https://www.baidu.com")
		return None
	
	# ä½¿ç”¨çŠ¶æ€æ˜¾ç¤º
	if not output.silent_mode:
		with Status("[bold green]ğŸ” Fetching target page...", console=output.console):
			html_raw = Extract_html(url)
	else:
		html_raw = Extract_html(url)
		
	if html_raw == None: 
		output.print_error(f"Cannot access {url}")
		return None
	
	output.print_verbose("ğŸ” Starting to parse HTML content...")
	html = BeautifulSoup(html_raw, "html.parser")
	
	# é¦–å…ˆä»HTMLæ ‡ç­¾ä¸­æå–URL
	output.print_verbose("ğŸ“‹ Extracting URLs from HTML attributes...")
	html_urls = URLExtractor.extract_urls_from_html(html_raw)
	output.print_verbose(f"ğŸ“‹ Found {len(html_urls)} URLs in HTML attributes")
	
	# ç„¶åå¤„ç†JavaScript
	html_scripts = html.find_all("script")
	output.print_verbose(f"ğŸ“„ Found {len(html_scripts)} script tags")
	
	script_array = {}
	script_temp = ""
	
	# åˆ›å»ºè¿›åº¦æ¡æ¥æ˜¾ç¤ºè„šæœ¬å¤„ç†è¿›åº¦
	progress = output.create_progress()
	if progress:
		with progress:
			script_task = progress.add_task("[cyan]ğŸ“„ Processing scripts...", total=len(html_scripts))
			
			for html_script in html_scripts:
				try:
					# æ£€æŸ¥æ˜¯å¦ä¸ºTagå¯¹è±¡
					if hasattr(html_script, 'get'):
						script_src = html_script.get("src")
						if script_src == None:
							script_temp += html_script.get_text() + "\n"
						else:
							purl = URLProcessor.process_url(url, script_src)
							progress.update(script_task, description=f"[cyan]ğŸ“„ Fetching: {purl.split('/')[-1]}")
							script_content = Extract_html(purl)
							if script_content:
								script_array[purl] = script_content
							else:
								output.print_warning(f"Cannot get external script: {purl}")
					else:
						# å¦‚æœä¸æ˜¯Tagå¯¹è±¡ï¼Œç›´æ¥è·å–æ–‡æœ¬å†…å®¹
						script_temp += html_script.get_text() + "\n"
				except AttributeError:
					# å¦‚æœå¯¹è±¡æ²¡æœ‰getæ–¹æ³•ï¼Œç›´æ¥è·å–æ–‡æœ¬å†…å®¹
					script_temp += html_script.get_text() + "\n"
				
				progress.advance(script_task)
	else:
		# é™é»˜æ¨¡å¼æˆ–æ— è¿›åº¦æ¡æ—¶çš„å¤„ç†
		for html_script in html_scripts:
			try:
				# æ£€æŸ¥æ˜¯å¦ä¸ºTagå¯¹è±¡
				if hasattr(html_script, 'get'):
					script_src = html_script.get("src")
					if script_src == None:
						script_temp += html_script.get_text() + "\n"
					else:
						purl = URLProcessor.process_url(url, script_src)
						script_content = Extract_html(purl)
						if script_content:
							script_array[purl] = script_content
						else:
							output.print_warning(f"Cannot get external script: {purl}")
				else:
					# å¦‚æœä¸æ˜¯Tagå¯¹è±¡ï¼Œç›´æ¥è·å–æ–‡æœ¬å†…å®¹
					script_temp += html_script.get_text() + "\n"
			except AttributeError:
				# å¦‚æœå¯¹è±¡æ²¡æœ‰getæ–¹æ³•ï¼Œç›´æ¥è·å–æ–‡æœ¬å†…å®¹
				script_temp += html_script.get_text() + "\n"
	
	script_array[url] = script_temp
	
	# åˆ†æè„šæœ¬ä»¥æå–URL
	allurls = {}
	
	# å…ˆæ·»åŠ HTMLä¸­æå–çš„URLs
	if html_urls:
		allurls["HTML_attributes"] = html_urls
	
	total_scripts = len(script_array)
	
	if not output.silent_mode:
		output.print_info(f"ğŸ” [bold yellow]Analyzing {total_scripts} scripts for API endpoints...[/bold yellow]")
	
	progress = output.create_progress()
	if progress:
		with progress:
			analyze_task = progress.add_task("[green]ğŸ” Analyzing scripts...", total=total_scripts)
			
			for script in script_array:
				script_name = script.split('/')[-1] if '/' in script else script
				progress.update(analyze_task, description=f"[green]ğŸ” Analyzing: {script_name}")
				
				output.print_verbose(f"ğŸ” Analyzing script: {script}")
				temp_urls = URLExtractor.extract_urls(script_array[script])
				
				if len(temp_urls) == 0: 
					output.print_verbose("ğŸ” No URLs found")
				else:
					output.print_verbose(f"âœ… Found {len(temp_urls)} URLs")
					allurls[script] = temp_urls
				
				progress.advance(analyze_task)
	else:
		# é™é»˜æ¨¡å¼å¤„ç†
		for script in script_array:
			output.print_verbose(f"ğŸ” Analyzing script: {script}")
			temp_urls = URLExtractor.extract_urls(script_array[script])
			if len(temp_urls) == 0: 
				output.print_verbose("ğŸ” No URLs found")
			else:
				output.print_verbose(f"âœ… Found {len(temp_urls)} URLs")
				allurls[script] = temp_urls

	# æ·»åŠ å…¨å±€é”ä¿è¯è¾“å‡ºå’Œç»Ÿè®¡çš„çº¿ç¨‹å®‰å…¨
	print_lock = threading.Lock()
	stats_lock = threading.Lock()

	# å¤„ç†å‘ç°çš„URL
	total_urls = sum(len(urls) for urls in allurls.values())
	if total_urls > 0:
		output.print_info(f"ğŸ¯ [bold green]Found {total_urls} potential API endpoints. Testing them...[/bold green]")

		# çº¿ç¨‹å®‰å…¨çš„è¿›åº¦æ¡æ›´æ–°å‡½æ•°
		def safe_update_progress(progress, task, description=None):
			with print_lock:
				if description:
					progress.update(task, description=description)
				progress.advance(task)

		# çº¿ç¨‹å®‰å…¨çš„URLæ‰“å°
		def safe_print_url(url, source):
			with print_lock:
				output.print_url(url, source)

		# çº¿ç¨‹å®‰å…¨çš„è¯·æ±‚å¤„ç†
		def process_url(j, i, base_url):
			temp1 = urlparse(j)
			temp2 = urlparse(base_url)

			if temp1.netloc != urlparse("1").netloc:
				target_url = j
			else:
				target_url = temp2.scheme + "://" + temp2.netloc + j

			safe_print_url(target_url, i)
			try:
				# æ³¨æ„çº¿ç¨‹å®‰å…¨
				do_request(target_url)
			except Exception as e:
				with print_lock:
					output.print_error(f"Error testing {target_url}: {str(e)}")

		progress = output.create_progress()
		if progress:
			with progress:
				test_task = progress.add_task("[blue]ğŸŒ Testing endpoints...", total=total_urls)
				with ThreadPoolExecutor(max_workers=arg.threads) as executor:  # å¯æ ¹æ®éœ€è¦è°ƒæ•´çº¿ç¨‹æ•°
					futures = []
					for i in allurls:
						for j in allurls[i]:
							url_display = j[:50] + "..." if len(j) > 50 else j

							# æäº¤ä»»åŠ¡åˆ°çº¿ç¨‹æ± 
							future = executor.submit(
								process_url, j, i, url
							)
							futures.append(future)

							# æ›´æ–°è¿›åº¦æ¡æè¿°ï¼ˆéå¿…éœ€ï¼‰
							progress.update(test_task, description=f"[blue]ğŸŒ In queue: {url_display}")

					# åŠ¨æ€æ›´æ–°è¿›åº¦æ¡
					for future in as_completed(futures):
						safe_update_progress(progress, test_task)
		else:
			# é™é»˜æ¨¡å¼å¤„ç†
			with ThreadPoolExecutor(max_workers=10) as executor:
				futures = []
				for i in allurls:
					for j in allurls[i]:
						futures.append(executor.submit(
							process_url, j, i, url
						))

				# ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
				for future in as_completed(futures):
					pass
	else:
		output.print_warning("âš ï¸ No API endpoints discovered in the scanned content")

	# æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
	output.stats["total_urls"] = total_urls
	
	# æ·±åº¦æ‰«æï¼šå¦‚æœè¿˜æœ‰æ·±åº¦ï¼Œç»§ç»­æ‰«æå‘ç°çš„åŒåŸŸåURL
	if depth < deep_scan_manager.max_depth:
		all_discovered_urls = []
		for source, urls in allurls.items():
			all_discovered_urls.extend(urls)
		
		filtered_urls = deep_scan_manager.get_filtered_urls(all_discovered_urls)
		
		if filtered_urls:
			output.print_info(f"ğŸ” [bold yellow]Found {len(filtered_urls)} URLs for deep scan (depth {depth + 1})...[/bold yellow]")
			max_deep_scan_urls = 10
			if len(filtered_urls) > max_deep_scan_urls:
				output.print_warning(f"âš ï¸ Limiting deep scan to {max_deep_scan_urls} URLs (found {len(filtered_urls)})")
				filtered_urls = filtered_urls[:max_deep_scan_urls]

			for deep_url in filtered_urls:
				try:

					if not deep_url.startswith(('http://', 'https://')):
						parsed_base = urlparse(url)
						deep_url = f"{parsed_base.scheme}://{parsed_base.netloc}{deep_url}"
					
					output.print_verbose(f"ğŸ” Starting deep scan for: {deep_url}")
					find_by_url(deep_url, depth + 1, deep_scan_manager)
					
				except Exception as e:
					output.print_error(f"Error in deep scan for {deep_url}: {str(e)}")
					continue



# è®¾ç½®ä¸€ä¸ªä¸»å‡½æ•°ï¼Œæ–¹ä¾¿åç»­æ·»åŠ æ–°çš„åŠŸèƒ½
def main():
	"""ä¸»å‡½æ•°"""
	
	# é¦–å…ˆå¤„ç†æ›´æ–°æ£€æŸ¥
	if arg.update:
		with Status("[bold blue]ğŸ”„ Checking for updates...", console=output.console):
			UpdateManager.check_for_updates(force_update=True)
		sys.exit(0)
	else:
		with Status("[bold blue]ğŸ”„ Checking for updates...", console=output.console):
			UpdateManager.check_for_updates(force_update=False)

	if not arg.silent:
		show_logo()
	
	try:
		url = arg.url
		
		# æ˜¾ç¤ºä»£ç†æ¨¡å¼
		output.print_proxy_mode(do_proxys())

		# å¼€å§‹æ‰«æ
		output.print_info(f"ğŸš€ [bold green]Starting API endpoint scan...[/bold green]")
		find_by_url(url)
		
		if not output.silent_mode:
			if output.stats["api_endpoints"] > 0:
				output.print_info(f"ğŸ‰ [bold green]Scan completed! Found {output.stats['api_endpoints']} API endpoints.[/bold green]")
			else:
				output.print_info(f"âœ… [bold yellow]Scan completed. No API endpoints found.[/bold yellow]")
	
	except KeyboardInterrupt:
		output.print_warning("\nâš ï¸ Scan interrupted by user")
		sys.exit(1)
	except Exception as e:
		output.print_error(f"Error: {str(e)}")
		raise  # è®©Richçš„å¼‚å¸¸å¤„ç†å™¨å¤„ç†
	
	finally:
		output.print_stats()
		file_output.save_results(arg.url, arg)

if __name__ == '__main__':
	main()