"""
@date: 2025
@version: 0.3.1
@description: ç”¨äºæ‰«æAPIç«¯ç‚¹
"""

import random
import requests
from urllib.parse import urlparse
from bs4 import BeautifulSoup
import argparse
import time
import sys
import json
import os
from datetime import datetime
from .ua_manager import UaManager
from .utils import URLProcessor, URLExtractor, UpdateManager
from .i18n import i18n
import threading
import pyfiglet
from rich.console import Console
from rich.text import Text
from rich.panel import Panel
from rich.table import Table
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.align import Align
from rich.live import Live
from rich.status import Status
from rich.json import JSON
from rich.traceback import install
from rich.columns import Columns
from rich.rule import Rule

# å®‰è£…Richçš„å¼‚å¸¸å¤„ç†
install()

parser = argparse.ArgumentParser(description="Api-Finder v0.3")
parser.add_argument("-u", "--url", help=i18n.get('arg_url_help'), required=True)
parser.add_argument("-c", "--cookie", help=i18n.get('arg_cookie_help'))
parser.add_argument("-p", "--proxy", help=i18n.get('arg_proxy_help'))
parser.add_argument("-s", "--silent", action="store_true", help=i18n.get('arg_silent_help'))
parser.add_argument("-o", "--output", help=i18n.get('arg_output_help'))
parser.add_argument("-t", "--timeout", type=int, default=10, help=i18n.get('arg_timeout_help'))
parser.add_argument("-d", "--delay", type=float, default=0.5, help=i18n.get('arg_delay_help'))
parser.add_argument("-v", "--verbose", action="store_true", help=i18n.get('arg_verbose_help'))
parser.add_argument("-r", "--random", action="store_true", help=i18n.get('arg_random_help'))
parser.add_argument("-a", "--app", help=i18n.get('arg_app_help'), default='common')
parser.add_argument("-U", "--update", action="store_true", help=i18n.get('arg_update_help'))
arg = parser.parse_args()

# åˆå§‹åŒ–Rich Console (Initialize Rich Console)
console = Console()

# åˆå§‹åŒ–UAç®¡ç†å™¨ (Initialize UA Manager)
Uam = UaManager(arg.app, arg.random)

# ä½¿ç”¨Riché‡æ„çš„Logoæ˜¾ç¤º
def show_logo():
	"""ä½¿ç”¨Richå’Œpyfigletæ˜¾ç¤ºç²¾ç¾logo"""
	try:
		# ç”ŸæˆASCII art
		logo_text = pyfiglet.figlet_format("Api-Finder", font="slant")
		
		# åˆ›å»ºå¸¦é¢œè‰²çš„logoæ–‡æœ¬
		logo = Text(logo_text, style="cyan bold")
		
		# åˆ›å»ºé¡¹ç›®ä¿¡æ¯æ–‡æœ¬
		info_text = Text()
		info_text.append("API Endpoint Scanner v0.5", style="green bold")
		info_text.append("     Github: github.com/jujubooom/Api-Finder\n", style="blue")
		info_text.append("Developed by jujubooom,bx,orxiain", style="green bold")
		
		# åˆ›å»ºé¢æ¿
		logo_panel = Panel(
			Align.center(logo),
			title="[yellow bold]ğŸš€ API-Finder ğŸš€[/yellow bold]",
			border_style="cyan",
			padding=(1, 2)
		)
		
		info_panel = Panel(
			Align.center(info_text),
			border_style="green",
			padding=(0, 2)
		)
		
		# æ˜¾ç¤ºlogoå’Œä¿¡æ¯
		console.print(logo_panel)
		console.print(info_panel)
		console.print(Rule(style="dim"))
		
	except Exception as e:
		# æ€¥æ•‘æªæ–½ - ä½¿ç”¨ç®€å•çš„Richæ˜¾ç¤º
		console.print(Panel(
			"[cyan bold]Api-Finder v0.3[/cyan bold]\n"
			"[blue]Github: github.com/jujubooom/Api-Finder[/blue]",
			title="ğŸš€ API-Finder ğŸš€",
			border_style="cyan"
		))


# Richèµ‹èƒ½çš„è¾“å‡ºç®¡ç†å™¨ç±»
class OutputManager:
	"""
	ä½¿ç”¨Richåº“é‡æ„çš„OutputManagerç±»ï¼Œæä¾›æ›´ç¾è§‚çš„ç»ˆç«¯è¾“å‡º
	
	silent_mode: é™é»˜æ¨¡å¼ï¼Œåªè¾“å‡ºå‘ç°çš„APIç«¯ç‚¹ (Silent mode, only output discovered API endpoints)
	verbose_mode: è¯¦ç»†è¾“å‡ºæ¨¡å¼ (Verbose output mode)
	output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ (Output file path)
	results: ç»“æœåˆ—è¡¨ (Results list)
	stats: ç»Ÿè®¡ä¿¡æ¯ (Statistics)
	"""
	def __init__(self, silent_mode, verbose_mode=False, output_file=None):
		self.silent_mode = silent_mode
		self.verbose_mode = verbose_mode
		self.output_file = output_file
		self.console = console  # ä½¿ç”¨å…¨å±€çš„Rich console
		self.results = []
		self.stats = {
			"total_urls": 0,
			"successful_requests": 0,
			"failed_requests": 0,
			"api_endpoints": 0,
			"start_time": datetime.now()
		}
		self.results_table = Table(title="ğŸ” Discovered API Endpoints", border_style="green")
		self.results_table.add_column("ğŸ“ URL", style="cyan", no_wrap=False)
		self.results_table.add_column("ğŸ“„ Source", style="yellow", max_width=30)
		self.results_table.add_column("â° Time", style="dim", max_width=10)
	
	def print_info(self, text):
		if not self.silent_mode:
			self.console.print(text)
	
	def print_verbose(self, text):
		if self.verbose_mode and not self.silent_mode:
			self.console.print(f"[dim][DEBUG][/dim] {text}")
	
	def print_url(self, url, source=""):
		if self.silent_mode:
			# é™é»˜æ¨¡å¼ä½¿ç”¨Richçš„printè€Œä¸æ˜¯æ™®é€šprint
			self.console.print(url, highlight=False)
		else:
			# æ·»åŠ åˆ°ç»“æœè¡¨æ ¼
			source_display = source.split('/')[-1] if source else "unknown"
			time_display = datetime.now().strftime("%H:%M:%S")
			self.results_table.add_row(url, source_display, time_display)
			
			if source:
				self.console.print(f"[green bold]âœ“[/green bold] [blue]{url}[/blue] [dim](from: {source_display})[/dim]")
			else:
				self.console.print(f"[green bold]âœ“[/green bold] [blue]{url}[/blue]")
		
		# ä¿å­˜ç»“æœ (Save results)
		self.results.append({
			"url": url,
			"source": source,
			"timestamp": datetime.now().isoformat()
		})
		self.stats["api_endpoints"] += 1
	
	def print_error(self, text):
		if not self.silent_mode:
			self.console.print(f"[red bold]âœ—[/red bold] {text}")
	
	def print_warning(self, text):
		if not self.silent_mode:
			self.console.print(f"[yellow bold]âš [/yellow bold] {text}")
	
	def print_success(self, text):
		if not self.silent_mode:
			self.console.print(f"[green bold]âœ“[/green bold] {text}")

	def print_title(self, url, title):
		"""æ‰“å°æˆåŠŸè¯·æ±‚çš„é¡µé¢æ ‡é¢˜"""
		if not self.silent_mode:
			text = Text()
			text.append("ğŸ“„ ", style="green")
			text.append(f"{title}", style="yellow")
			text.append(" â†’ ", style="dim")
			text.append(f"{url}", style="cyan dim")
			self.console.print(text)

	# è¾“å‡ºä½¿ç”¨çš„ä»£ç†æ¨¡å¼ (Output proxy mode used)
	def print_proxy_mode(self, proxies):
		if not self.silent_mode:
			if proxies:
				proxy_table = Table(title="ğŸŒ Proxy Configuration", border_style="blue")
				proxy_table.add_column("Type", style="cyan")
				proxy_table.add_column("Address", style="green")
				
				if isinstance(proxies, list):
					for proxy in proxies:
						proxy_table.add_row("SOCKS5", proxy)
				elif isinstance(proxies, dict):
					for protocol, proxy in proxies.items():
						proxy_table.add_row(protocol.upper(), proxy)
				
				self.console.print(proxy_table)
			else:
				self.console.print("[yellow]ğŸ’» Direct connection (no proxy)[/yellow]")
			self.console.print(Rule(style="dim"))

	def print_stats(self):
		if not self.silent_mode:
			# è®¡ç®—æ‰«ææ—¶é—´
			scan_duration = datetime.now() - self.stats["start_time"]
			duration_str = f"{scan_duration.total_seconds():.1f}s"
			
			# åˆ›å»ºç»Ÿè®¡è¡¨æ ¼
			stats_table = Table(title="ğŸ“Š Scan Statistics", border_style="cyan")
			stats_table.add_column("Item", style="yellow bold")
			stats_table.add_column("Value", style="green bold", justify="right")
			
			stats_table.add_row("ğŸ¯ Total URLs", str(self.stats['total_urls']))
			stats_table.add_row("âœ… Successful Requests", str(self.stats['successful_requests']))
			stats_table.add_row("âŒ Failed Requests", str(self.stats['failed_requests']))
			stats_table.add_row("ğŸ” API Endpoints Found", str(self.stats['api_endpoints']))
			stats_table.add_row("â±ï¸ Scan Duration", duration_str)
			
			# è®¡ç®—æˆåŠŸç‡
			total_requests = self.stats['successful_requests'] + self.stats['failed_requests']
			if total_requests > 0:
				success_rate = (self.stats['successful_requests'] / total_requests) * 100
				stats_table.add_row("ğŸ“ˆ Success Rate", f"{success_rate:.1f}%")
			
			self.console.print(Rule(style="dim"))
			self.console.print(stats_table)
			
			# å¦‚æœæ‰¾åˆ°äº†APIç«¯ç‚¹ï¼Œæ˜¾ç¤ºç»“æœè¡¨æ ¼
			if self.stats['api_endpoints'] > 0 and not self.silent_mode:
				self.console.print(Rule(style="dim"))
				self.console.print(self.results_table)
	
	def save_results(self):
		if not self.output_file:
			return
		
		try:
			file_ext = os.path.splitext(self.output_file)[1].lower()
			
			if file_ext == '.json':
				with open(self.output_file, 'w', encoding='utf-8') as f:
					json.dump({
						"scan_info": {
							"target_url": arg.url,
							"scan_time": datetime.now().isoformat(),
							"stats": {
								**self.stats,
								"start_time": self.stats["start_time"].isoformat()
							}
						},
						"results": self.results
					}, f, ensure_ascii=False, indent=2)
			
			elif file_ext == '.txt':
				with open(self.output_file, 'w', encoding='utf-8') as f:
					f.write(f"{i18n.get('output_header')}\n")
					f.write(f"{i18n.get('output_target')}: {arg.url}\n")
					f.write(f"{i18n.get('output_scan_time')}: {datetime.now().isoformat()}\n")
					f.write(f"{i18n.get('output_endpoints_found')}: {self.stats['api_endpoints']}\n")
					f.write("-" * 50 + "\n")
					for result in self.results:
						f.write(f"{result['url']}\n")
			
			elif file_ext == '.csv':
				import csv
				with open(self.output_file, 'w', newline='', encoding='utf-8') as f:
					writer = csv.writer(f)
					writer.writerow(['URL', 'Source', 'Timestamp'])
					for result in self.results:
						writer.writerow([result['url'], result['source'], result['timestamp']])
			
			if not self.silent_mode:
				self.console.print(f"\n[green bold]ğŸ’¾ Results saved to:[/green bold] [blue]{self.output_file}[/blue]")
				
		except Exception as e:
			self.print_error(f"Save failed: {str(e)}")
	
	def create_progress(self, total_tasks=None):
		"""åˆ›å»ºè¿›åº¦æ¡"""
		if self.silent_mode:
			return None
		
		return Progress(
			SpinnerColumn(),
			TextColumn("[progress.description]{task.description}"),
			BarColumn(),
			TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
			TimeElapsedColumn(),
			console=self.console,
			expand=True
		)

# åˆå§‹åŒ–è¾“å‡ºç®¡ç†å™¨ (Initialize output manager)
output = OutputManager(arg.silent, arg.verbose, arg.output)
proxies_global = None

def do_proxys():
	global proxies_global
	
	if proxies_global is not None:
		return proxies_global
	
	if arg.proxy == "0":
		# è‡ªåŠ¨è·å–ä»£ç†åˆ—è¡¨ (Auto fetch proxy list)
		header = {"User-Agent": Uam.getUa()}
		proxy_response = requests.get("https://proxy.scdn.io/api/get_proxy.php?protocol=socks5&count=5", headers=header).text
		proxy_data = json.loads(proxy_response)
		if proxy_data.get("code") == 200 and "data" in proxy_data and "proxies" in proxy_data["data"]:
			proxies_global = proxy_data["data"]["proxies"]
		else:
			output.print_error(i18n.get('proxy_fetch_failed'))
			proxies_global = []

	elif arg.proxy:
		# åˆ¤æ–­ä»£ç†ç±»å‹æ˜¯å¦ä¸ºsocks5
		if arg.proxy.startswith('socks5://'):
			proxies_global = {
				"http": arg.proxy,
				"https": arg.proxy
			}
		# æ™®é€šhttp/httpsä»£ç†
		else:
			proxies_global = {
				"http": arg.proxy if arg.proxy.startswith('http') else f'http://{arg.proxy}',
				"https": arg.proxy if arg.proxy.startswith('http') else f'http://{arg.proxy}'
			}
	
	return proxies_global

# åˆ›å»ºçº¿ç¨‹å®‰å…¨çš„ç»“æœå­˜å‚¨ç»“æ„ (Create thread-safe result storage structure)
class ResultStore:
	def __init__(self):
		self.results = {"GET": {}, "POST": {}}
		self.lock = threading.Lock()

	def update(self, method, success, response_text, error=None):
		with self.lock:
			self.results[method] = {
				"success": success,
				"response": response_text,
				"error": error
			}


# è¯·æ±‚æ‰§è¡Œå‡½æ•° (Request execution function)
def make_request(method, url, cookies, timeout, store):
	# è¯·æ±‚å‰çš„é…ç½® (Request configuration)
	proxies = do_proxys()
	if proxies and isinstance(proxies, list):
		proxies = {
			"socks5": proxies[random.randint(0,len(proxies)-1)],
		}
	header = {"User-Agent": Uam.getUa()}

	try:
		if method == "GET":
			if proxies:
				res = requests.get(url, headers=header, cookies=cookies,
								   timeout=timeout, proxies=proxies)
			else:
				res = requests.get(url, headers=header, cookies=cookies,
								   timeout=timeout)
		else:  # POST
			if proxies:
				res = requests.post(url, headers=header, cookies=cookies,
								   timeout=timeout, proxies=proxies)
			else:
				res = requests.post(url, headers=header, cookies=cookies,
								   timeout=timeout)

		res.raise_for_status()
		response_text = res.text.replace(" ", "").replace("\n", "")
		store.update(method, True, response_text)

	except requests.exceptions.RequestException as e:
		store.update(method, False, None, str(e))
	except Exception as e:
		store.update(method, False, None, str(e))


def do_request(url):
	result_store = ResultStore()

	# åˆ›å»ºå¹¶å¯åŠ¨çº¿ç¨‹
	get_thread = threading.Thread(
		target=make_request,
		args=("GET", url, {"Cookie": arg.cookie}, arg.timeout, result_store)
	)

	post_thread = threading.Thread(
		target=make_request,
		args=("POST", url, {"Cookie": arg.cookie}, arg.timeout, result_store)
	)

	# å¯åŠ¨çº¿ç¨‹
	get_thread.start()
	post_thread.start()

	# ç­‰å¾…ä¸¤ä¸ªçº¿ç¨‹å®Œæˆ
	get_thread.join()
	post_thread.join()
	
	response_text_to_return = None

	# ç»Ÿä¸€è¾“å‡ºç»“æœ (Unified output results)
	for method in ["GET", "POST"]:
		result = result_store.results[method]
		if result["success"]:
			response_text = result['response']
			
			if method == "GET":
				response_text_to_return = response_text
				# å°è¯•è§£æå’Œæ‰“å°æ ‡é¢˜
				try:
					if response_text and '<html' in response_text.lower():
						soup = BeautifulSoup(response_text, 'html.parser')
						if soup.title and soup.title.string:
							title = soup.title.string.strip().replace('\\n', '').replace('\\r', '')
							if title:
								output.print_title(url, title)
				except Exception as e:
					output.print_verbose(f"Could not parse title from {url}: {e}")

			if method == "GET" and output.silent_mode:
				output.console.print(url, highlight=False)
			elif not output.silent_mode:
				output.print_success(f"{method} request successful for {url}")
				if output.verbose_mode:
					res_len = len(response_text)
					output.print_verbose(f"ğŸ“ Response length: {res_len} characters")
					output.print_verbose(f"ğŸ‘€ Response preview: {response_text[:200]}...")

			output.stats["successful_requests"] += 1
		else:
			output.print_error(f"{method} request failed for {url}: {result['error']}")
			output.stats["failed_requests"] += 1
	
	# è¯·æ±‚é—´éš”
	time.sleep(arg.delay)
	return response_text_to_return


# è·å–HTMLå†…å®¹ (Extract HTML content)
def Extract_html(URL):
	"""
	URL: ç›®æ ‡URL (Target URL)
	header: è¯·æ±‚å¤´ (Request headers)
	raw: è¯·æ±‚è¿”å›çš„å†…å®¹ (Raw response content)
	content: è§£æåçš„HTMLå†…å®¹ (Parsed HTML content)
	return: è¿”å›HTMLå†…å®¹ (Return HTML content)
	"""
	header = {"User-Agent": Uam.getUa()}
	try:
		raw = requests.get(URL, headers=header, timeout=arg.timeout, cookies=arg.cookie)
		raw.raise_for_status()
		content = raw.content.decode("utf-8", "ignore")
		output.print_verbose(f"âœ… Successfully retrieved HTML content: {URL}")
		return content
	except requests.exceptions.RequestException as e:
		output.print_error(f"Failed to get HTML {URL}: {str(e)}")
		return None
	except Exception as e:
		output.print_error(f"HTML extraction exception {URL}: {str(e)}")
		return None


def find_by_url(url):
	try:
		output.print_info(f"ğŸ¯ [bold blue]Starting scan target:[/bold blue] [green]{url}[/green]")
	except:
		output.print_info("âŒ Please specify a valid URL, e.g.: https://www.baidu.com")
		return None
	
	# ä½¿ç”¨çŠ¶æ€æ˜¾ç¤º
	if not output.silent_mode:
		with Status("[bold green]ğŸ” Fetching target page...", console=output.console):
			html_raw = Extract_html(url)
	else:
		html_raw = Extract_html(url)
	
	if html_raw == None: 
		output.print_error(f"Cannot access {url}")
		return None
	
	output.print_verbose("ğŸ” Starting to parse HTML content...")
	html = BeautifulSoup(html_raw, "html.parser")
	html_scripts = html.findAll("script")
	output.print_verbose(f"ğŸ“„ Found {len(html_scripts)} script tags")
	
	script_array = {}
	script_temp = ""
	
	# åˆ›å»ºè¿›åº¦æ¡æ¥æ˜¾ç¤ºè„šæœ¬å¤„ç†è¿›åº¦
	progress = output.create_progress()
	if progress:
		with progress:
			script_task = progress.add_task("[cyan]ğŸ“„ Processing scripts...", total=len(html_scripts))
			
			for html_script in html_scripts:
				script_src = html_script.get("src")
				if script_src == None:
					script_temp += html_script.get_text() + "\n"
				else:
					purl = URLProcessor.process_url(url, script_src)
					progress.update(script_task, description=f"[cyan]ğŸ“„ Fetching: {purl.split('/')[-1]}")
					script_content = Extract_html(purl)
					if script_content:
						script_array[purl] = script_content
					else:
						output.print_warning(f"Cannot get external script: {purl}")
				
				progress.advance(script_task)
	else:
		# é™é»˜æ¨¡å¼æˆ–æ— è¿›åº¦æ¡æ—¶çš„å¤„ç†
		for html_script in html_scripts:
			script_src = html_script.get("src")
			if script_src == None:
				script_temp += html_script.get_text() + "\n"
			else:
				purl = URLProcessor.process_url(url, script_src)
				script_content = Extract_html(purl)
				if script_content:
					script_array[purl] = script_content
				else:
					output.print_warning(f"Cannot get external script: {purl}")
	
	script_array[url] = script_temp
	
	# åˆ†æè„šæœ¬ä»¥æå–URL
	allurls = {}
	total_scripts = len(script_array)
	
	if not output.silent_mode:
		output.print_info(f"ğŸ” [bold yellow]Analyzing {total_scripts} scripts for API endpoints...[/bold yellow]")
	
	progress = output.create_progress()
	if progress:
		with progress:
			analyze_task = progress.add_task("[green]ğŸ” Analyzing scripts...", total=total_scripts)
			
			for script in script_array:
				script_name = script.split('/')[-1] if '/' in script else script
				progress.update(analyze_task, description=f"[green]ğŸ” Analyzing: {script_name}")
				
				output.print_verbose(f"ğŸ” Analyzing script: {script}")
				temp_urls = URLExtractor.extract_urls(script_array[script])
				
				if len(temp_urls) == 0: 
					output.print_verbose("ğŸ” No URLs found")
				else:
					output.print_verbose(f"âœ… Found {len(temp_urls)} URLs")
					allurls[script] = temp_urls
				
				progress.advance(analyze_task)
	else:
		# é™é»˜æ¨¡å¼å¤„ç†
		for script in script_array:
			output.print_verbose(f"ğŸ” Analyzing script: {script}")
			temp_urls = URLExtractor.extract_urls(script_array[script])
			if len(temp_urls) == 0: 
				output.print_verbose("ğŸ” No URLs found")
			else:
				output.print_verbose(f"âœ… Found {len(temp_urls)} URLs")
				allurls[script] = temp_urls
	
	# å¤„ç†å‘ç°çš„URL
	total_urls = sum(len(urls) for urls in allurls.values())
	if total_urls > 0:
		output.print_info(f"ğŸ¯ [bold green]Found {total_urls} potential API endpoints. Testing them...[/bold green]")
		
		progress = output.create_progress()
		if progress:
			with progress:
				test_task = progress.add_task("[blue]ğŸŒ Testing endpoints...", total=total_urls)
				
				for i in allurls:
					for j in allurls[i]:
						# æ˜¾ç¤ºå½“å‰æ­£åœ¨æµ‹è¯•çš„URL
						url_display = j[:50] + "..." if len(j) > 50 else j
						progress.update(test_task, description=f"[blue]ğŸŒ Testing: {url_display}")
						
						output.print_url(j, i)
						temp1 = urlparse(j)
						temp2 = urlparse(url)
						
						if temp1.netloc != urlparse("1").netloc:
							do_request(j)
						else:
							do_request(temp2.scheme+"://"+temp2.netloc+j)
						
						progress.advance(test_task)
		else:
			# é™é»˜æ¨¡å¼å¤„ç†
			for i in allurls:
				for j in allurls[i]:
					output.print_url(j, i)
					temp1 = urlparse(j)
					temp2 = urlparse(url)
					
					if temp1.netloc != urlparse("1").netloc:
						do_request(j)
					else:
						do_request(temp2.scheme+"://"+temp2.netloc+j)
	else:
		output.print_warning("âš ï¸ No API endpoints discovered in the scanned content")
	
	# æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
	output.stats["total_urls"] = total_urls



# è®¾ç½®ä¸€ä¸ªä¸»å‡½æ•°ï¼Œæ–¹ä¾¿åç»­æ·»åŠ æ–°çš„åŠŸèƒ½
def main():
	"""ä¸»å‡½æ•°"""
	
	# é¦–å…ˆå¤„ç†æ›´æ–°æ£€æŸ¥
	if arg.update:
		with Status("[bold blue]ğŸ”„ Checking for updates...", console=output.console):
			UpdateManager.check_for_updates(force_update=True)
		sys.exit(0)
	else:
		with Status("[bold blue]ğŸ”„ Checking for updates...", console=output.console):
			UpdateManager.check_for_updates(force_update=False)

	if not arg.silent:
		show_logo()
	
	try:
		url = arg.url
		
		# æ˜¾ç¤ºä»£ç†æ¨¡å¼
		output.print_proxy_mode(do_proxys())

		# å¼€å§‹æ‰«æ
		output.print_info(f"ğŸš€ [bold green]Starting API endpoint scan...[/bold green]")
		results = find_by_url(url)
		
		if not output.silent_mode:
			if output.stats["api_endpoints"] > 0:
				output.print_info(f"ğŸ‰ [bold green]Scan completed! Found {output.stats['api_endpoints']} API endpoints.[/bold green]")
			else:
				output.print_info(f"âœ… [bold yellow]Scan completed. No API endpoints found.[/bold yellow]")
	
	except KeyboardInterrupt:
		output.print_warning("\nâš ï¸ Scan interrupted by user")
		sys.exit(1)
	except Exception as e:
		output.print_error(f"Error: {str(e)}")
		raise  # è®©Richçš„å¼‚å¸¸å¤„ç†å™¨å¤„ç†
	
	finally:
		output.print_stats()
		output.save_results()

if __name__ == '__main__':
	main()